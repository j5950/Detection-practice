{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Reference: https://github.com/jwyang/faster-rcnn.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code of faster-rcnn\n",
    " - Goal: understand overall training flow & check whether decreases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'ANCHOR_RATIOS': [0.5, 1, 2],\n",
      " 'ANCHOR_SCALES': [8, 16, 32],\n",
      " 'CROP_RESIZE_WITH_MAX_POOL': False,\n",
      " 'CUDA': True,\n",
      " 'DATA_DIR': '/home/cvlab/junha/git/faster-rcnn.pytorch/data',\n",
      " 'DEDUP_BOXES': 0.0625,\n",
      " 'EPS': 1e-14,\n",
      " 'EXP_DIR': 'vgg16',\n",
      " 'FEAT_STRIDE': [16],\n",
      " 'GPU_ID': 0,\n",
      " 'MATLAB': 'matlab',\n",
      " 'MAX_NUM_GT_BOXES': 20,\n",
      " 'MOBILENET': {'DEPTH_MULTIPLIER': 1.0,\n",
      "               'FIXED_LAYERS': 5,\n",
      "               'REGU_DEPTH': False,\n",
      "               'WEIGHT_DECAY': 4e-05},\n",
      " 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),\n",
      " 'POOLING_MODE': 'align',\n",
      " 'POOLING_SIZE': 7,\n",
      " 'RESNET': {'FIXED_BLOCKS': 1, 'MAX_POOL': False},\n",
      " 'RNG_SEED': 3,\n",
      " 'ROOT_DIR': '/home/cvlab/junha/git/faster-rcnn.pytorch',\n",
      " 'TEST': {'BBOX_REG': True,\n",
      "          'HAS_RPN': True,\n",
      "          'MAX_SIZE': 1000,\n",
      "          'MODE': 'nms',\n",
      "          'NMS': 0.3,\n",
      "          'PROPOSAL_METHOD': 'gt',\n",
      "          'RPN_MIN_SIZE': 16,\n",
      "          'RPN_NMS_THRESH': 0.7,\n",
      "          'RPN_POST_NMS_TOP_N': 300,\n",
      "          'RPN_PRE_NMS_TOP_N': 6000,\n",
      "          'RPN_TOP_N': 5000,\n",
      "          'SCALES': [600],\n",
      "          'SVM': False},\n",
      " 'TRAIN': {'ASPECT_GROUPING': False,\n",
      "           'BATCH_SIZE': 256,\n",
      "           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n",
      "           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n",
      "           'BBOX_NORMALIZE_TARGETS': True,\n",
      "           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,\n",
      "           'BBOX_REG': True,\n",
      "           'BBOX_THRESH': 0.5,\n",
      "           'BG_THRESH_HI': 0.5,\n",
      "           'BG_THRESH_LO': 0.0,\n",
      "           'BIAS_DECAY': False,\n",
      "           'BN_TRAIN': False,\n",
      "           'DISPLAY': 10,\n",
      "           'DOUBLE_BIAS': True,\n",
      "           'FG_FRACTION': 0.25,\n",
      "           'FG_THRESH': 0.5,\n",
      "           'GAMMA': 0.1,\n",
      "           'HAS_RPN': True,\n",
      "           'IMS_PER_BATCH': 1,\n",
      "           'LEARNING_RATE': 0.01,\n",
      "           'MAX_SIZE': 1000,\n",
      "           'MOMENTUM': 0.9,\n",
      "           'PROPOSAL_METHOD': 'gt',\n",
      "           'RPN_BATCHSIZE': 256,\n",
      "           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'RPN_CLOBBER_POSITIVES': False,\n",
      "           'RPN_FG_FRACTION': 0.5,\n",
      "           'RPN_MIN_SIZE': 8,\n",
      "           'RPN_NEGATIVE_OVERLAP': 0.3,\n",
      "           'RPN_NMS_THRESH': 0.7,\n",
      "           'RPN_POSITIVE_OVERLAP': 0.7,\n",
      "           'RPN_POSITIVE_WEIGHT': -1.0,\n",
      "           'RPN_POST_NMS_TOP_N': 2000,\n",
      "           'RPN_PRE_NMS_TOP_N': 12000,\n",
      "           'SCALES': [600],\n",
      "           'SNAPSHOT_ITERS': 5000,\n",
      "           'SNAPSHOT_KEPT': 3,\n",
      "           'SNAPSHOT_PREFIX': 'res101_faster_rcnn',\n",
      "           'STEPSIZE': [30000],\n",
      "           'SUMMARY_INTERVAL': 180,\n",
      "           'TRIM_HEIGHT': 600,\n",
      "           'TRIM_WIDTH': 600,\n",
      "           'TRUNCATED': False,\n",
      "           'USE_ALL_GT': True,\n",
      "           'USE_FLIPPED': True,\n",
      "           'USE_GT': False,\n",
      "           'WEIGHT_DECAY': 0.0005},\n",
      " 'USE_GPU_NMS': True}\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "dataset_name = 'pascal_voc'\n",
    "net = 'vgg16'\n",
    "start_epoch = 1\n",
    "max_epochs = 20\n",
    "disp_interval = 100\n",
    "checkpoint_interval = 10000\n",
    "save_dir = 'models'\n",
    "num_workers = 0\n",
    "cuda = True\n",
    "large_scale = False               \n",
    "mGPUs = False\n",
    "batch_size = 2\n",
    "class_agnostic = False\n",
    "\n",
    "# config optimization\n",
    "optimizer = \"sgd\"\n",
    "lr = 0.001\n",
    "lr_decay_step = 5\n",
    "lr_decay_gamma = 0.1\n",
    "\n",
    "# set training session\n",
    "session = 1\n",
    "\n",
    "# resume trained model\n",
    "resume = False\n",
    "checksession = 1\n",
    "checkepoch = 1\n",
    "checkpoint = 0\n",
    "\n",
    "# log and display\n",
    "use_tfboard = False\n",
    "\n",
    "imdb_name = \"voc_2007_trainval\"\n",
    "imdbval_name = \"voc_2007_test\"\n",
    "set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n",
    "cfg_file = \"cfgs/{}_ls.yml\".format(net) if large_scale else \"cfgs/{}.yml\".format(net)\n",
    "\n",
    "if cfg_file is not None:\n",
    "    cfg_from_file(cfg_file)\n",
    "if set_cfgs is not None:\n",
    "    cfg_from_list(set_cfgs)\n",
    "\n",
    "cfg.TRAIN.USE_FLIPPED = True\n",
    "cfg.USE_GPU_NMS = cuda    \n",
    "cfg.CUDA = True\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "np.random.seed(cfg.RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Sampler class\n",
    " - argument of 'Dataloader' (sampler='')\n",
    " - define how to generate indices\n",
    " - if given, argument 'shuffle' should be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampler(Sampler):\n",
    "    def __init__(self, train_size, batch_size):\n",
    "        self.num_data = train_size\n",
    "        self.num_per_batch = int(train_size / batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
    "        self.leftover_flag = False\n",
    "        if train_size % batch_size:\n",
    "            self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "            self.leftover_flag = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
    "        self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
    "\n",
    "        self.rand_num_view = self.rand_num.view(-1)\n",
    "\n",
    "        if self.leftover_flag:\n",
    "            self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
    "\n",
    "        return iter(self.rand_num_view)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train dataloader object\n",
    "<br>\n",
    " (1) Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `voc_2007_trainval` for training\n",
      "Set proposal method: gt\n",
      "Appending horizontally-flipped training examples...\n",
      "voc_2007_trainval gt roidb loaded from /home/cvlab/junha/git/faster-rcnn.pytorch/data/cache/voc_2007_trainval_gt_roidb.pkl\n",
      "done\n",
      "Preparing training data...\n",
      "Image sizes loaded from /home/cvlab/junha/git/faster-rcnn.pytorch/data/cache/voc_2007_trainval_sizes.pkl\n",
      "done\n",
      "before filtering, there are 10022 images...\n",
      "after filtering, there are 10022 images...\n",
      "10022 roidb entries\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "# -- Note: Use validation set and disable the flipped to enable faster loading.\n",
    "cfg.TRAIN.USE_FLIPPED = True\n",
    "cfg.USE_GPU_NMS = cuda\n",
    "imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name)\n",
    "train_size = len(roidb)\n",
    "\n",
    "print('{:d} roidb entries'.format(len(roidb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) initialize dataset & sampler object <br>\n",
    "(3) get dataloader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_batch = sampler(train_size, batch_size)\n",
    "\n",
    "dataset = roibatchLoader(roidb, ratio_list, ratio_index, batch_size, \\\n",
    "                       imdb.num_classes, training=True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                        sampler=sampler_batch, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3977e+14])\n"
     ]
    }
   ],
   "source": [
    "# initilize the input tensor holder here.\n",
    "im_data = torch.FloatTensor(1)\n",
    "im_info = torch.FloatTensor(1)\n",
    "num_boxes = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "print(num_boxes)\n",
    "\n",
    "# ship to cuda\n",
    "if cuda:\n",
    "    im_data = im_data.cuda()\n",
    "    im_info = im_info.cuda()\n",
    "    num_boxes = num_boxes.cuda()\n",
    "    gt_boxes = gt_boxes.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize object detection network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from data/pretrained_model/vgg16_caffe.pth\n"
     ]
    }
   ],
   "source": [
    "# initilize the network here.\n",
    "if net == 'vgg16':\n",
    "    fasterRCNN = vgg16(imdb.classes, pretrained=True, class_agnostic=class_agnostic)\n",
    "fasterRCNN.create_architecture()\n",
    "\n",
    "if cuda:\n",
    "    fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get parameters of model\n",
    " - we can set different value of learning rate, weight_decay params, etc.\n",
    " - we can filter params whoose requires_grad is False (freezing network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for key, value in dict(fasterRCNN.named_parameters()).items():\n",
    "    if value.requires_grad:\n",
    "        if 'bias' in key:\n",
    "            params += [{'params':[value],'lr':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n",
    "                        'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n",
    "        else:\n",
    "            params += [{'params':[value],'lr':lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n",
    "optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training iteration\n",
    " - get input data\n",
    " - forward\n",
    " - get loss\n",
    " - backward\n",
    " - save checkpoint (model weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[session 1][epoch  1][iter    0/2505] loss: 4.3540, lr: 1.00e-03\n",
      "\t\t\tfg/bg=(110/914), time cost: 0.554538\n",
      "\t\t\trpn_cls: 0.6332, rpn_box: 0.1427, rcnn_cls: 3.2903, rcnn_box 0.2877\n",
      "[session 1][epoch  1][iter  100/2505] loss: 1.3776, lr: 1.00e-03\n",
      "\t\t\tfg/bg=(67/957), time cost: 55.616909\n",
      "\t\t\trpn_cls: 0.1685, rpn_box: 0.0559, rcnn_cls: 0.3553, rcnn_box 0.1442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-012d0df2c4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrpn_loss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_loss_box\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mRCNN_loss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRCNN_loss_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mrois_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasterRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpn_loss_cls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrpn_loss_box\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/samsung3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/junha/git/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im_data, im_info, gt_boxes, num_boxes)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# feed base feature map tp RPN to obtain rois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_loss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_loss_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCNN_rpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# if it is training phrase, then use ground trubut bboxes for refining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/samsung3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/junha/git/faster-rcnn.pytorch/lib/model/rpn/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, base_feat, im_info, gt_boxes, num_boxes)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data,\n\u001b[0;32m---> 78\u001b[0;31m                                  im_info, cfg_key))\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_loss_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/samsung3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/junha/git/faster-rcnn.pytorch/lib/model/rpn/proposal_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     83\u001b[0m         shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n\u001b[1;32m     84\u001b[0m                                   shift_x.ravel(), shift_y.ravel())).transpose())\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mshifts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshifts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir = save_dir + \"/\" + net + \"/\" + dataset_name\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "iters_per_epoch = int(train_size / batch_size)\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs + 1):\n",
    "    # setting to train mode\n",
    "    fasterRCNN.train()\n",
    "    loss_temp = 0\n",
    "    start = time.time()\n",
    "\n",
    "    if epoch % (lr_decay_step + 1) == 0:\n",
    "        adjust_learning_rate(optimizer, lr_decay_gamma)\n",
    "        lr *= lr_decay_gamma\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        im_data.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_info.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxes.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        '''\n",
    "        Your code below\n",
    "        \n",
    "        1. forward path\n",
    "        arguemnt & return values of forward() function(defined in 'lib/model/faster_rcnn/faster_rcnn.py')\n",
    "        argument: im_data, im_info, gt_boxes, num_boxes\n",
    "        return values: rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_bbox, RCNN_loss_cls, RCNN_loss_bbox, rois_label\n",
    "        \n",
    "        2. define total loss term\n",
    "\n",
    "        3. backward path\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        loss_temp += loss.item()\n",
    "        if step % disp_interval == 0:\n",
    "            end = time.time()\n",
    "            if step > 0:\n",
    "                loss_temp /= (disp_interval + 1)\n",
    "\n",
    "            loss_rpn_cls = rpn_loss_cls.item()\n",
    "            loss_rpn_box = rpn_loss_box.item()\n",
    "            loss_rcnn_cls = RCNN_loss_cls.item()\n",
    "            loss_rcnn_box = RCNN_loss_bbox.item()\n",
    "            fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "            bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "\n",
    "            print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
    "                                    % (session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
    "            print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
    "            print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
    "                          % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
    "\n",
    "            loss_temp = 0\n",
    "            start = time.time()\n",
    "\n",
    "    save_name = os.path.join(output_dir, 'faster_rcnn_{}_{}_{}.pth'.format(session, epoch, step))\n",
    "    save_checkpoint({\n",
    "        'session': session,\n",
    "        'epoch': epoch + 1,\n",
    "        'model': fasterRCNN.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'pooling_mode': cfg.POOLING_MODE,\n",
    "        'class_agnostic': class_agnostic,\n",
    "    }, save_name)\n",
    "    print('save model: {}'.format(save_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After checking loss decreasing, \n",
    " (0) Before running demo, please make sure whether images/ directory is empty. (remove '.ipynb_checkpoints/' or any other files/dirs)<br>\n",
    " (1) put your own images which contains at least one person into images/ directory <br>\n",
    " (2) type 'python demo.py --net vgg16 --checksession 1 --checkepoch $EPOCH --checkpoint 1669 --cuda --load_dir models' <br>\n",
    " on root dir of project <br>\n",
    " (3) check the result of object detection at images/ directory\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
